{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Ma9T_aLfBf"
      },
      "source": [
        "<img src=\"https://github.com/isardmart/01MIAR_ABR_24/blob/main/img/viu_logo.png?raw=1\" width=\"200\">\n",
        "\n",
        "# 01MIAR - Actividad Whitepapers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**"
      ],
      "metadata": {
        "id": "mU0kOmxTRLxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link del [PDF](https://arxiv.org/pdf/2005.11401v3)"
      ],
      "metadata": {
        "id": "GgugyjeMLjUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducción**\n",
        "\n",
        "El artículo \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" presenta un enfoque innovador para mejorar las tareas de procesamiento de lenguaje natural (NLP) mediante la combinación de modelos generativos preentrenados con técnicas de recuperación de información. Este método permite generar respuestas más precisas y contextualmente relevantes en tareas como preguntas abiertas y verificación de hechos.\n"
      ],
      "metadata": {
        "id": "SxUnN7xjRVTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resumen del Artículo**\n",
        "\n",
        "El enfoque de Generación Aumentada por Recuperación (RAG) combina modelos generativos preentrenados, como BART y T5, con técnicas avanzadas de recuperación de documentos para mejorar la precisión y relevancia de las respuestas en tareas de NLP.\n",
        "\n",
        "RAG utiliza representaciones vectoriales densas para recuperar documentos relevantes de una base de datos y condiciona las respuestas generadas en esta información recuperada.\n",
        "\n",
        "Este método ha demostrado ser eficaz en tareas como la respuesta a preguntas abiertas y la verificación de hechos, siendo especialmente útil en ámbitos como la investigación médica y el soporte al cliente. RAG representa un avance significativo en NLP al combinar los puntos fuertes de los modelos generativos y los sistemas de recuperación de información.\n"
      ],
      "metadata": {
        "id": "yNOvVkb4Rfdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Ejemplo usando un modelo t5**\n",
        "\n",
        "**Transformers (Hugging Face)**\n",
        "\n",
        "**Uso:** Implementación de modelos preentrenados como BART y T5 para tareas de generación de texto.\n",
        "\n"
      ],
      "metadata": {
        "id": "QtAt7qRISROT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dA72M3qSX8PG",
        "outputId": "fd259ff2-baa9-4330-fffb-496779dfb1e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conseguir un token en hugging face debes registrarte y pedir un token de api gratuito"
      ],
      "metadata": {
        "id": "SSISGIAGG0jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import login\n",
        "login(\"HUGGING_FACE_TOKEN\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w4Sd1OEUXMbG",
        "outputId": "da6e8514-53bc-4272-87ad-5cc1b11081cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Carregar el model i el tokenitzador\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Text d'entrada llarg per resumir\n",
        "input_text = \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) is a technique in natural language processing that combines generative models, such as T5 or BART, with retrieval mechanisms to produce more accurate and contextually relevant responses. The key idea is to enhance the generative model's ability to produce answers by first retrieving relevant documents or passages from a large corpus of text, and then conditioning the generation process on this retrieved information. This approach leverages both parametric knowledge, encoded in the model's parameters, and non-parametric knowledge, found in the external text corpus.\n",
        "\n",
        "The process typically involves two main steps. First, a retrieval system, often based on dense vector representations and similarity search algorithms like FAISS, identifies the most relevant pieces of text from a pre-indexed database. This step ensures that the information used to generate the response is up-to-date and contextually appropriate. Second, the generative model uses the retrieved text as additional context to generate a more precise and informative response. By incorporating retrieval into the generation process, RAG models can significantly improve performance on tasks such as question answering, fact-checking, and summarization.\n",
        "\n",
        "RAG models have shown remarkable success in various knowledge-intensive NLP tasks. They excel in situations where the model needs to access a large amount of information that it cannot feasibly store in its parameters alone. For example, in open-domain question answering, a RAG model can retrieve the most relevant articles or snippets from a vast collection of documents and then generate a coherent and accurate answer based on this retrieved content. This capability makes RAG models particularly useful in domains like healthcare, legal research, and customer support, where accessing the latest and most relevant information is crucial.\n",
        "\n",
        "Overall, Retrieval-Augmented Generation represents a significant advancement in the field of NLP, providing a robust framework for creating more reliable and context-aware language models. By effectively combining retrieval and generation, RAG models are able to bridge the gap between purely generative models and traditional information retrieval systems, offering a powerful tool for a wide range of applications.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocesamiento del texto para T5\n",
        "input_ids = tokenizer(\"summarize: \" + input_text, return_tensors='pt').input_ids\n",
        "\n",
        "# Generación de texto amb paràmetres ajustats\n",
        "outputs = model.generate(input_ids, max_length=100, num_beams=10, early_stopping=True)\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21VCsvL9hFX_",
        "outputId": "90bd810e-71f7-445c-a3a4-82c8ff2cbc75",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieval-augmented generation (RAG) is a technique in natural language processing. it combines generative models with retrieval mechanisms to produce more accurate answers. the technique leverages both parametric knowledge and non-parametric knowledge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código utiliza el modelo T5 para resumir un texto largo. Una vez instaladas las librerias, carga el modelo t5-base y su tokenizador desde la librería transformers. El modelo t5-base es una versión del modelo T5 preentrenado, adecuado para tareas de generación de texto.\n",
        "\n",
        "Definimos un texto largo que explica el concepto de Generación Aumentada por Recuperación (RAG).\n",
        "\n",
        "El texto de entrada se preprocesa añadiendo la instrucción \"summarize: \" antes del texto y luego se tokeniza usando el tokenizador T5. El resultado es un tensor que puede ser utilizado por el modelo T5.\n",
        "\n",
        "El modelo genera un resumen del texto de entrada utilizando los parámetros especificados:\n",
        "\n",
        "max_length=100: La longitud máxima del resumen generado.\n",
        "\n",
        "num_beams=10: Número de haces en la búsqueda por haz, que ayuda a encontrar mejores resultados.\n",
        "\n",
        "early_stopping=True: Detiene la generación cuando se alcanza una condición de parada temprana.\n",
        "\n"
      ],
      "metadata": {
        "id": "iEc9wM0QG-vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo del uso de FAISS**\n",
        "\n",
        "**Uso:** Indexación y recuperación rápida de documentos relevantes durante el proceso de generación.\n",
        "\n"
      ],
      "metadata": {
        "id": "oGxTVB3sSdOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Crear un índice FAISS\n",
        "dimension = 128  # Dimensión del vector de característiques\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Crear vectores de muestra\n",
        "vectors = np.random.random((1000, dimension)).astype('float32')\n",
        "\n",
        "# Añadir los vectors a l'índex\n",
        "index.add(vectors)\n",
        "\n",
        "# Vector de consulta\n",
        "query_vector = np.random.random((1, dimension)).astype('float32')\n",
        "\n",
        "# Buscar los 5 vectores más propers\n",
        "k = 5\n",
        "distances, indices = index.search(query_vector, k)\n",
        "\n",
        "print(\"Índices de vecinos cercanos:\", indices)\n",
        "print(\"Distancias a índices cercanos:\", distances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqUxIWGKSkVR",
        "outputId": "d2a66cb4-2a06-4c4a-fd95-df3ba641a60d",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indices of the nearest neighbors: [[340  55 378 505 510]]\n",
            "Distances to the nearest neighbors: [[14.242166 14.665246 15.033352 15.197968 15.282605]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código utiliza la librería FAISS para indexar y buscar vectores de características en un espacio de dimensiones específicas. Primero, importa las librerías necesarias: faiss para la indexación y búsqueda de vectores, y numpy para la creación de datos numéricos.\n",
        "\n",
        "Se define la dimensión de los vectores de características como 128. A continuación, se crea un índice FAISS utilizando la distancia Euclidiana (L2) para medir la similitud entre los vectores.\n",
        "\n",
        "Luego, se generan 1000 vectores de muestra con 128 dimensiones, con valores aleatorios entre 0 y 1, utilizando numpy. Estos vectores se convierten al tipo float32 y se añaden al índice FAISS.\n",
        "\n",
        "Para realizar una búsqueda, se genera un vector de consulta también de 128 dimensiones con valores aleatorios entre 0 y 1. Este vector se utiliza para buscar los 5 vectores más cercanos en el índice FAISS.\n",
        "\n",
        "Finalmente, se realiza la búsqueda de los 5 vectores más cercanos al vector de consulta. El resultado de la búsqueda incluye los índices de los vectores más cercanos y las distancias Euclidianas entre el vector de consulta y estos vectores. Estos resultados se imprimen para mostrar los índices y las distancias de los vectores más cercanos encontrados.\n",
        "\n"
      ],
      "metadata": {
        "id": "s3Ou_lmZHy6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión**\n",
        "\n",
        "El enfoque de Generación Aumentada por Recuperación (RAG) es un gran avance en el campo del procesamiento de lenguaje natural (NLP). Combina modelos generativos avanzados, como BART y T5, con técnicas de recuperación de documentos. Utilizando librerías potentes como Transformers y FAISS, RAG mejora significativamente la precisión y relevancia de las respuestas en tareas complejas de NLP.\n",
        "\n",
        "Sin embargo, RAG tiene sus desafíos. Depender de grandes volúmenes de datos externos para la recuperación puede causar problemas de escalabilidad y eficiencia, especialmente en aplicaciones en tiempo real. Además, la combinación del conocimiento almacenado en los modelos y del extraído de documentos puede llevar a inconsistencias en las respuestas, especialmente si los documentos recuperados varían en calidad.\n",
        "\n",
        "Otro punto crítico es la interpretabilidad de los modelos RAG. La complejidad de mezclar múltiples fuentes de información hace difícil entender cómo y por qué se generan ciertas respuestas, lo cual es crucial en áreas sensibles como la medicina o el derecho. Además, depender de modelos preentrenados puede introducir sesgos y problemas de equidad en las respuestas generadas.\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Disponible en [arXiv](https://arxiv.org/abs/2005.11401v3).\n",
        "\n",
        "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J. (2020). Transformers: State-of-the-Art Natural Language Processing. [Github/huggingface/transformers](https://github.com/huggingface/transformers)\n",
        "\n",
        "Johnson, J., Douze, M., & Jégou, H. (2017). Billion-scale similarity search with GPUs. Disponible en arXiv. [arXiv](https://arxiv.org/pdf/1702.08734)"
      ],
      "metadata": {
        "id": "LcOdd8_hSwkH"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}